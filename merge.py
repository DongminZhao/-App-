from crawler import *
import os


def file_name_walk(file_dir):
    temp_file = ''
    for files in os.walk(file_dir):
        print("files", files[2])
        temp_file = files
    return temp_file[2]
# 遍历目录


def get_data():
    apps = [
        # 360 apple MI
        [48269  , 423514795 ,2086   ,"工商银行"],
        [9347   , 391965015 ,195    , "建设银行"],
        [35383  , 399608199 ,2081   , "中国银行"],
        [169212 , 515651240 ,9407   , "农业银行"],
        [3574796, 493489515 ,466518 , "邮储银行"],
        [6885   , 392899425 ,778    , "招商银行"],
        [3282633, 1434388720,399634 , "交通银行"],
        [3183249, 523091708 ,163871 , "民生银行"],
        [3080418, 433592972 ,121535 , "兴业银行"],
        [1026   , 422844108 ,10649  , "中信银行"],
        [34716  , 471855847 ,14083  , "浦发银行"],
        [2370721, 415872960 ,79554  , "广发银行"],
        [3260425, 1085016815,428633 , "平安口袋银行"],
        [792546 , 666188756 ,766427 , "华夏银行"],
        [221900 , 398453262 ,19936  , "掌上生活(招行)"],
        [738318 , 518566498 ,57522  , "江苏银行"],
        [260788 , 631875777 ,105708 , "上海银行"],
        [3067502, 994103968 ,107136 , "微众银行"],
        [121819 , 509982592 ,188988 , "广州农商银行"],
        [3950072, 447733826 ,558119 , "光大银行"],
        [1586020, 791490969 ,82510  , "苏州银行"],
        [203990 , 485543769 ,70632  , "重庆银行"],
        [1766717, 784245616 ,466506 , "东莞银行"],
        [2524119, 834214932 ,97914  , "西安银行"],
        [327566 , 535140314 ,None   , "兰州银行"],
        [203986 , 448578672 ,804026 , "昆仑银行"],
        [298202 , 557825942 ,64442  , "渤海银行"],
        [298196 , 487994173 ,265056 , "浙商银行"],
        [298200 , 557825942 ,None   , "天津银行"],
        [None   , 515667501 ,737221 , "四川农信银行"],
        [None   , 1315576144,786583 , "衡水银行"],
        [None   , 1037966533,535786 , "宁夏银行"],
        # [None   , 1076471425,None , "西藏银行"],
        [None   , 435060143 ,20211  , "南京银行"],
        [233240 , 949829082 , 62404 , "北京银行"],
    ]
    print("Start")
    for app in apps:
        crawler_360(app[0], app[3])
        crawler_apple(app[1], app[3])
        crawler_mi(app[2], app[3])
# 爬取数据


def data_merge(file_dir=r'./crawler/raw_data'):
    data = ""
    for temp in file_name_walk(file_dir):
        with open(r'./data/'+temp, 'r', encoding='utf-8') as f:
            data += f.read()+'\n'
    with open(r'./总评论.txt', 'w', encoding='utf-8') as f:
        f.write(data)
# 合并爬取数据


# if __name__ == '__main__':
#     get_data()
#     data_merge()
